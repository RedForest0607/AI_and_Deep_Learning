# 선형회귀
***
### 1. 데이터에 대한 이해
훈련 데이터 셋과 테스트 데이터 셋

예측을 위해 사용하는 데이터셋 = **훈련 데이터셋**

학습이 끝난 후 얼마나 잘 작동하는지 판별하는 데이터셋을 = **테스트 데이터셋**

### 2. 훈련 데이터셋의 구성
~~~python
x_train = torch.FloatTensor([[1], [2], [3]])
y_train = torch.FloatTensor([[1], [2], [3]])
~~~

각각의 1*3 크기의 행렬의 데이터를 매핑합니다. x는 입력, y는 출력을 의미합니다

### Hypothesis 가설의 수립
선형회귀의 가설은 학습 데이터와 가장 잘 맞는 직선을 찾는 것 입니다.

y = Wx +b

와 같은 직선의 식을 가설로 세웁니다.

W는 가중치이며, b는 bias(편향)이라고 합니다.

### 비용 함수
***
비용 함수 = 손실 함수 = 오차 함수 = 목적 함수

훈련 데이터 사이에서 직선을 그었을 때, 훈련 데이터 를 전부 지나지 못하고, 근소하게 벗어나는 경우가 생깁니다.
이를 최소화하기 위해서 오차를 계산해야하는데, 총 오차를 계산하기 위해서 모든 오차를 단순히 덧셈 하게 되면,
음수와 양수가 섞여서 올바르게 값을 알 수 없습니다.

따라서 오차를 전부 제곱하여 계산하게 됩니다.

그리고 오차의 제곱합의 평균을 구한 것을 **평균 제곱 오차**(Mean Squared Error,MSE)
라고 합니다.


### 옵티마이저 - 경사 하강법
***
비용함수를 최적화 하기 위한 가중치를 찾는 방버빙 바로 **경사 하강법(Gradient Descent)**입니다.
이는 미분과 비슷한 개념인데, 가중치가 너무 높아져도, 낮아져도 Cost가 무한대가 되어 버리니, 
가장 Cost가 낮은 근사값의 기울기를 찾는 **미분**과 비슷한 방식으로 가장 적은 가중치를 찾아냅니다.

이러한 가중치에다가 특정한 a(학습률)을 계속해서 곱해가면서 구합니다. 이러한 학습률은, 너무 작으면 많은 학습을 해야하고,
반대로 학습률이 너무 크다면 발산해 버리는 OverShoot이 일어나게 됩니다.