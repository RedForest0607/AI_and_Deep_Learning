# 인공 신경망
***
# Logistic Regression 로지스틱 회귀
## Binary Classification
이진 분류는 두가지의 선택지 중에서 하나를 선택하는 선택 방법 입니다.
이러한 선택 방법은 Linear Regression과 다르게 일자가 아닌 S자 형태의 그래프를 필요로 합니다.
## Sigmoid Function
이러한 이진 분류를 위한 그래프를 그려주는 함수가 바로 Sigmoid Function 입니다.
Linear Regression에서 최적의 가중치와 편향을 찾는 것 처럼, Binary Classification에서는
W값에 따라서 S자 가운데의 경사도가 변합니다. W의 값이 커질 수록 경사도가 올라갑니다. 또한 b값의 변화로 그래프가 전체적으로 좌우로 움직이게 됩니다.

###  퍼셉트론
***
 전통적인 인공 신경망중에서 딥 러닝의 바탕이 된 퍼셉트론은, 사람의 뇌의 뉴런과 같이 여러개의 퍼셉트론들이 각각의 입력과 출력을
 받아서, 입력값 x는 가중치 W와 함께 인공 뉴런들에게 전달되어서 가중치의 곱의 전체 합이 Threshold를 넘으면 종단에서 출력신호로
 1을 출력합니다.
 
기본적으로 퍼셉트론은 1과 0의 값을 뱉어내는 계단 함수로써

단층 퍼셉트론은 Tensor에 입력값을 입력 받고, 값을 출력하는 두 단계로 이루어 집니다.
이러한 단계를 층이라고 부르며, 단층 퍼셉트론은 입력층과 출력층으로 나뉩니다. 
문제는 이러한 단층 퍼셉트론은 논리회로에서 결과값을 나눌 수 있는 선이 하나이기 때문에 XOR 문제를 풀 수 없습니다.
그래서 여러개의 레이어를 사용한 다층 퍼셉트론을 사용합니다.
#
이러한 신경망 학습은 입력에 대해서 Forward Propagation을 통해서 예측값과 실제값의 오차를 손실 함수로 계산하고, 그 손실을 미둔을 통해서 Gradient(기울기)
를 구하고, 이를 통해서 역전파를 계산하게 됩니다.  
단, 이진 분류를 위한 Sigmoid 함수가 오히려 DNN에서는 역효과를 낳게 됩니다. Sigmoid 함수의 1도 0도 아닌 기울기 부분이 은닉층을 계속 통과하면서 기울기가 점점 0에 가까워지면서
값이 제대로 넘어가지 않아서 문제가 발생하게 됩니다.

### 하이퍼볼릭탄젠트 함수
  tanh 함수는 입력값을 -1과 1 사이로 바꾸게 됩니다. 시그모이드 함수와 비슷한 그래프 형태여서 같은 문제가 생기는 경우가 있지만, 0이 중심이기 때문에 0 - 1인
sigmoid 함수 보다 변화폭이 큽니다. 따라서 기울기 소실이 덜 일어나는 편이게 됩니다.
### ReLU 렐루 함수
~~~
f(x) = max(0, x)
def relu(x):
 return np.maximum(1, x)
~~~
 렐루 함수는 인공 신경망에서 가장 자주 사용되는 함수 입니다. 렐루 함수는 음수를 입력하면 0을 출력하고 양수를 입력하면 입력값을 그대로 반환합니다. 수렴하는 부분이 없기 때문에 Sigmoid 
함수보다 훨씬 더 잘 작동하게 됩니다. 단순 임계값이므로 연산 속도도 빠르게 됩니다.  
단, 입력값이 음수인 경우, 기울기 또한 0이 되며, 이 기울기 값으로 인한 죽은 뉴런이 나타나게 됩니다. (dying ReLU probelm)

### 리키 렐루(Leaky ReLU)
 리키 렐루는 죽은 렐루를 보안하기 위해서 입력값이 음수 일 경우 0이 아닌 0.01등의 매우 작은 수로 적용합니다.
~~~
f(x) = max(ax,x)
~~~
a는 하이퍼리스트로써 0.01과 같이 Leaky(새는) 정도를 뜻합니다.

### 소프트맥스 함수
은닉층에서 ReLU등이 사용되지만, 시그모이드나 소프트맥스 함수도 사용합니다. 분류 문제를 로지스틱 회귀나 소프트맥스 회귀등을 사용하는 경우도 있습니다.
시그모이드가 이진 분류로 사용도니다면, 소프트 맥스는 다중 클래스 분류애 사용합니다.
### 다층 퍼셉트론
***
